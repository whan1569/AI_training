1. 실험 목표
이 실험은 ReLU(Rectified Linear Unit) 활성화 함수를 사용하여 CIFAR-10 데이터셋을 분류하는 모델을 학습하는 과정과 결과를 분석하는 것을 목표로 합니다. 또한, 학습 중간중간 진행 상황을 시각화하고, ReLU 활성화 함수가 특징 맵을 어떻게 변화시키는지 분석합니다.

2. 데이터셋
- 데이터셋 이름: CIFAR-10
- 내용: CIFAR-10은 10개의 클래스로 구성된 컬러 이미지 데이터셋으로, 각 클래스는 6,000개의 이미지로 이루어져 있습니다. 각 이미지는 32x32 픽셀 크기입니다.

3. 모델 아키텍처
모델은 2개의 Conv2D 층과 2개의 MaxPooling2D 층, 1개의 Fully Connected 층으로 구성되었습니다. 각 Conv2D 층의 활성화 함수로 ReLU가 사용되었으며, 최종 출력은 softmax를 사용하여 10개의 클래스로 분류합니다.

모델 구조:
- Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3))
- MaxPooling2D()
- Conv2D(64, (3, 3), activation='relu')
- MaxPooling2D()
- Flatten()
- Dense(64, activation='relu')
- Dense(10, activation='softmax')

4. 학습 방법
- 최적화 알고리즘: Adam
- 손실 함수: sparse_categorical_crossentropy
- 에포크 수: 10
- 배치 크기: 64

5. 실험 과정
1. CIFAR-10 데이터셋을 로드하고 전처리합니다. 모든 이미지는 0과 1 사이로 정규화됩니다.
2. 모델을 정의하고 컴파일합니다.
3. `TrainingVisualization` 콜백 함수를 정의하여 학습 중 손실 값과 정확도를 실시간으로 그래프에 표시합니다.
4. 모델을 학습하고, 매 에포크마다 훈련 손실과 정확도, 검증 손실과 정확도를 기록하고, 이를 시각화합니다.
5. `visualize_feature_maps` 함수를 사용하여 Conv2D 계층에서 추출된 특징 맵을 시각화합니다.

6. 실험 결과
- 훈련 정확도: 매 에포크마다 훈련 정확도가 점차 증가했습니다. 첫 번째 에포크에서 40% 정도였던 정확도는 최종적으로 80% 이상으로 향상되었습니다.
- 검증 정확도: 검증 데이터셋에서도 비슷한 패턴을 보였으며, 최종 검증 정확도는 약 78%로 안정적인 성능을 보였습니다.
- 손실 함수: 손실 값은 학습 초기에 급격히 감소했으며, 에포크가 진행됨에 따라 점차 완만해졌습니다.

7. 특징 맵 시각화
- 학습된 모델에서 첫 번째 Conv2D 층과 두 번째 Conv2D 층에서 추출된 특징 맵을 시각화했습니다. ReLU 함수는 음수 값을 0으로 변환하여 음성 신호를 제거하는 특성을 보여주었고, 이는 모델이 이미지에서 중요한 패턴을 더욱 잘 인식하는 데 도움을 주었습니다.
  
8. 결론
이 실험을 통해 ReLU 활성화 함수가 CIFAR-10 이미지 분류 문제에서 중요한 역할을 한다는 것을 확인할 수 있었습니다. ReLU는 음수 값을 차단하여 모델이 더 빠르게 학습할 수 있게 하며, 학습 속도와 성능을 향상시킬 수 있음을 확인했습니다. 또한, 특징 맵 시각화를 통해 ReLU가 데이터에서 중요한 특징을 추출하는 과정을 직관적으로 이해할 수 있었습니다.

9. 향후 작업
- 모델 튜닝: 다른 활성화 함수(예: Leaky ReLU, ELU)와 비교하여 성능을 개선할 수 있을지 실험할 예정입니다.
- 데이터 증강: 데이터 증강을 통해 모델의 일반화 능력을 높이고, 과적합을 방지할 수 있을지 실험할 계획입니다.