
ReLU (Rectified Linear Unit)는 딥러닝에서 주로 사용되는 활성화 함수 중 하나로, 입력값이 0 이하일 경우에는 0을 출력하고, 0보다 크면 그대로 출력하는 함수입니다. 즉, 수학적으로는 다음과 같이 표현할 수 있습니다:

    ReLU(x) = max(0, x)

이 함수의 주요 특징은 **비선형성**을 가지면서도 계산이 매우 간단하고 효율적이라는 점입니다. 그 결과, ReLU는 신경망에서 널리 사용되며, 특히 **딥러닝** 모델에서 매우 중요한 역할을 합니다.

### ReLU의 장점:
1. **계산 효율성**: ReLU는 계산이 간단하고 빠릅니다. 복잡한 연산을 필요로 하지 않아 학습 속도가 빨라집니다.
2. **Vanishing Gradient 문제 해결**: Sigmoid나 Tanh 함수와 비교했을 때, ReLU는 **기울기 소실(vanishing gradient) 문제**를 해결하는 데 도움이 됩니다. 입력값이 크면 그라디언트가 일정하게 유지되므로, 역전파 과정에서 가중치가 적절히 업데이트됩니다.
3. **희소성(sparsity)**: ReLU는 0 이하의 값에 대해서 0을 출력하므로, 출력값이 0인 뉴런이 많아지는 경향이 있습니다. 이는 **희소성**을 촉진시켜, 모델이 더욱 효율적으로 학습되도록 합니다.

### ReLU의 단점:
1. **죽은 ReLU 문제 (Dead ReLU)**: 입력값이 0 이하일 때 출력값이 0이 되기 때문에, 일부 뉴런이 계속 0을 출력하게 되어 **죽은 뉴런**이 발생할 수 있습니다. 이렇게 되면 해당 뉴런은 학습이 되지 않아서 네트워크가 제대로 학습되지 않을 수 있습니다. 이를 해결하기 위해 **Leaky ReLU**, **ELU**와 같은 변형들이 사용됩니다.

### ReLU 변형:
1. **Leaky ReLU**: ReLU의 단점을 해결하려는 방법으로, 입력값이 0 이하일 때도 작은 기울기를 허용하는 함수입니다. 수식은 다음과 같습니다:
       Leaky ReLU(x) = max(αx, x)
   여기서 α는 작은 상수값입니다.

2. **Parametric ReLU (PReLU)**: Leaky ReLU의 확장으로, α를 학습 가능한 파라미터로 두어, 모델이 최적화할 수 있도록 합니다.

3. **ELU (Exponential Linear Unit)**: ReLU와 비슷하지만, 입력값이 0 이하일 때 지수 함수를 사용하여 출력값이 음수가 될 수 있도록 설계되었습니다.

ReLU는 그 간단함과 효율성 덕분에 대부분의 현대 신경망에서 활성화 함수로 널리 사용되고 있습니다.
