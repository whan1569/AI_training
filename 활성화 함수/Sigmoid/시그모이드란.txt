시그모이드 함수 (Sigmoid Function) 분석 보고서

1. 개요
시그모이드 함수는 주로 신경망, 로지스틱 회귀 및 확률적 분류 문제에서 사용되는 활성화 함수입니다. 
이 함수는 주어진 입력값을 0과 1 사이의 값으로 변환하는 특성을 가지고 있으며, 
확률을 표현하거나 이진 분류 문제에서 클래스의 확률을 예측하는 데 유용합니다.

2. 수학적 정의
시그모이드 함수는 다음과 같이 정의됩니다:

σ(x) = 1 / (1 + e^(-x))

여기서:
- e는 자연상수(약 2.718)입니다.
- x는 함수의 입력값입니다.

3. 주요 특징
- 출력 범위: 시그모이드 함수는 출력값이 0과 1 사이에 있도록 변환합니다. 따라서 확률을 표현할 때 자주 사용됩니다. 예를 들어, 이진 분류 문제에서 각 클래스에 대한 확률을 출력할 때 유용합니다.
- 모양: 시그모이드 함수의 그래프는 'S'자 모양을 하고 있습니다. 입력값이 매우 커지거나 매우 작아지면 출력값은 각각 1 또는 0에 가까워지며, 중간값에서는 급격히 변합니다.
- 미분 가능: 시그모이드 함수는 미분 가능하며, 그 미분값은 함수 자체에 기반하여 계산할 수 있습니다. 이는 신경망에서 역전파 시 가중치를 업데이트하는 데 중요합니다.

4. 시그모이드 함수의 미분
시그모이드 함수의 미분은 다음과 같습니다:

σ'(x) = σ(x)(1 - σ(x))

즉, 출력값이 σ(x)라면, 그 미분값은 σ(x)와 (1 - σ(x))의 곱입니다. 이 특성 덕분에 역전파 시 계산이 효율적입니다.

5. 사용 분야
- 이진 분류: 시그모이드 함수는 출력값을 0과 1 사이로 변환하므로, 두 클래스 중 하나를 예측하는 데 사용됩니다. 예를 들어, 특정 입력이 클래스 1일 확률을 출력할 때 사용됩니다.
- 신경망: 시그모이드 함수는 은닉층의 활성화 함수로도 사용될 수 있습니다.

6. 단점
- Vanishing Gradient: 시그모이드 함수는 입력값이 너무 크거나 작을 때 출력이 0 또는 1에 매우 가까워지므로, 그로 인한 기울기 소실 문제가 발생할 수 있습니다. 이 문제는 학습 속도를 느리게 하고, 깊은 신경망에서는 잘 작동하지 않게 만듭니다.
- 비대칭성: 출력값이 0과 1 사이에만 존재하므로, 출력의 중심이 0이 아니고 0.5로 비대칭적입니다. 이는 데이터의 분포와 맞지 않을 수 있습니다.

7. 결론
시그모이드 함수는 과거의 신경망 모델에서 많이 사용되었으나, 현재는 ReLU와 같은 다른 활성화 함수들이 더 많이 사용되고 있습니다. 
그럼에도 불구하고 이진 분류와 확률적 예측을 수행하는 문제에서는 여전히 중요한 역할을 하고 있습니다.
