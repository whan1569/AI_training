최적화 알고리즘 목록

1. 경사 하강법 (Gradient Descent)
   - 설명: 함수의 기울기를 따라 최소값을 찾는 방법.
   - 사용처: 선형 회귀, 로지스틱 회귀, 신경망 학습 등.
   - 변형:
     - 배치 경사 하강법 (Batch Gradient Descent)
     - 확률적 경사 하강법 (Stochastic Gradient Descent, SGD)
     - 미니배치 경사 하강법 (Mini-batch Gradient Descent)
     - 모멘텀, AdaGrad, RMSProp, Adam 등.

2. 유전자 알고리즘 (Genetic Algorithm)
   - 설명: 자연선택 및 유전학을 모방하여 최적화 문제를 해결.
   - 사용처: 복잡한 탐색 공간을 가진 문제 (조합 최적화 문제 등).

3. 시뮬레이티드 어닐링 (Simulated Annealing)
   - 설명: 열역학에서 온도를 천천히 낮추며 안정 상태에 도달하는 과정에 착안한 알고리즘.
   - 사용처: 일정 최적화, 경로 최적화 문제 등.

4. 인공 신경망 (Artificial Neural Networks)
   - 설명: 복잡한 함수나 패턴을 모델링하는 방법으로, 최적화 과정에서 경사 하강법을 이용해 가중치를 조정.
   - 사용처: 이미지 분류, 자연어 처리, 음성 인식 등.

5. 입자 군집 최적화 (Particle Swarm Optimization, PSO)
   - 설명: 군집 지능을 이용하여 최적화 문제를 해결하는 방법으로, 여러 입자가 최적해를 찾기 위해 협력하는 방식.
   - 사용처: 연속적인 최적화 문제, 다중 최적화 문제 등.

6. 탐색적 최적화 기법 (Exploratory Optimization)
   - 설명: 예를 들어 랜덤 서치, 그리드 서치 등으로 하이퍼파라미터 튜닝에 사용.
   - 사용처: 머신러닝 모델의 하이퍼파라미터 최적화.

7. 다익스트라 알고리즘 (Dijkstra’s Algorithm)
   - 설명: 그래프에서 두 점 사이의 최단 경로를 찾는 알고리즘.
   - 사용처: 네트워크 최적화, 교통 경로 최적화.

8. 베이지안 최적화 (Bayesian Optimization)
   - 설명: 확률 모델을 이용해 목적 함수를 근사하고 샘플링을 통해 최적화하는 방법.
   - 사용처: 고비용 함수의 최적화 (하이퍼파라미터 튜닝 등).

9. 혼합 정수 선형 프로그래밍 (Mixed Integer Linear Programming, MILP)
   - 설명: 선형 방정식 시스템을 최적화하는 방법으로, 결정 변수에 정수 제약이 있는 문제.
   - 사용처: 생산 계획, 스케줄링, 자원 배분 문제 등.

10. L-BFGS (Limited-memory Broyden-Fletcher-Goldfarb-Shanno)
    - 설명: 고차원 최적화 문제를 해결하는 방법으로, 메모리를 절약하며 BFGS 알고리즘을 사용.
    - 사용처: 머신러닝에서 파라미터 최적화, 대규모 최적화 문제.
